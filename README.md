# Natual-Language-Processing
the application of computational techniques to the analysis and synthesis of natural language and speech.
I have applied the Following Techniques in Chronological order to do Natural Language Processing
  1. Tokenization- The process of replacing sensitive data with unique identification symbols that retain all the essential information                       about the data without compromising its security.
  2. Stemming  -  In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived)                  words to their word stem, base or root form—generally a written word form. The stem need not be identical to the                          morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is                    not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search                    engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation.
  3. Lemmatization - Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of                         words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which                     is known as the lemma.
  4. Parts-of-Speech-Tagging- In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical                               tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as                                          corresponding to a particular part of speech, based on both its definition and its context.
  5. Named-entity recognition - (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.
  6. Bag of Words -  The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.
  7. Term Frequency, Inverse Document Frequency - In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.
